JESSICA CLAIRE

100 Montgomery St. 10th Floor
(555) 432-1000 - resumesample@example.com

Summary

 

Dedicated IT professional with 5+ years of experience in System design, development and delivery of enterprise software and Implementation
of complex Client/Server and Intraneti/web based n-tier architecture systems utilizing Object oriented and Internet technologies with strong
domain knowledge. Around 6 years of ETL experience in Database programming for creating Data Warehouse with IBM
InfosphereDataStage1 1.5/8.5. (Manager, Designer, Administrator, Director), Parallel Extender, Talend and Teradata 14.10/13.10/12/10, DB2
‘UDB. Experience in working with Talend open source, Talend Enterprise version and Talend cloud. Experience in working with Standard
jobs, Batch jobs and streaming jobs using Talend for Bigdata and Talend for real time Big Data. Experience with SDLC including analysis,
design, construction, testing, and implementation. Extracted data from multiple operational sources and implemented SCDs (Type 1/Type 2/
Type 3) using Talend. Extensively used Talend components like tfileinputdelimited, tparquetinput, tspakrow, tSetGlobal Var tMap, tReplicate,
tloin, tFileList, tSortRow, tBufferlnput, tBufferOutput, tDenormalize, tNormalize, tParseRecordSet, {UniqueRow, tS3put, tS3get, tS3FileList,
‘tRedshiftInput, tRedshiftOutput, tRedshiftRow,tsnowflakeinput, tsnowdlakeoutput, tsnowflakerow tKafkalnput, tKafkaOutput,
tAzurestorageput, tAzurestorageget, tAzureStorageConnection, tAzureStoragelist. Extensively worked on Error logging components like
tLogCatcher, tStatCatcher, tAssertCatcher, FlowMeter, tFlowMeterCatcher. Experience in Debugging, Error Handling and Performance
Tuning of sources, targets, Jobs etc. Well knowledge and experience in Hadoop ecosystem (HDFS, YARN, Hive, SQOOP, HBASE,) Data
pipeline, data analysis and processing with hive SQL. Experience in Importing and Exporting data using Sqoop from Relational database
system to HDFS, Hive tables, and vise-versa Implemented various projects on Agile Development &Waterfall model and hybrid model
Enhanced the design of current systems to improve system capabilities to meet the changing needs of the business. Modified application
systems and procedures to optimize functional requirements including capacity, operating time, response time, and form of desired results.
Strictly followed Title II of HIPAA, known as the Administrative Simplification (AS) provisions, security and privacy of health data.
Expertise in writing UNIX shell scripts and hands on experience with scheduling of shell scripts using ASG-ZENA. Excellent experience in
Relational Database (RDBMS) and ODS , Oracle 11g,10g,9i, Microsoft SQL Server, Netezza, Teradata Load and MultiLoad, SQL, PL/SQL,
TOAD. Created Business Requirement, functional, technical, and mapping design documents. Experience in designing Job Batches and Job
Sequences for scheduling server and parallel jobs using DataStage Director, UNIX scripts.

 

Sms
ETL Tools * Linux, Windows

+ Talend Data Integration, Talend Cloud, DataStage 11.5, 8.5 Scheduling

(Designer, Manager, Director), Informatica * DataStage Director, Zena, TMC

Databases & Tools Scripting

# Oracle (11g), Teradata v12, Hive. * Unix, SQL
Languages Reporting Tools

* SQL, C, PL/SQL, COBOL, Kom /C Shell Scripting, HTML, * Tableau

XML, MS-Visual Basic, IIS, ASP, Java Script, Python, VB Script
Operating Systems

EXPERIENCE

 

12/2020 to Current ETL Developer
Allegis Group — Jacksonville, FL

‘© Designed various jobs for extracting data from various sources involving flat files and relational tables

‘© Worked with the Data Team to understand the source to target mapping rules

‘* Analyzed the requirements and framed the business logic for the ETL process using Talend and involved in the
Talend jobs design and its Development

‘* Developed jobs using Talend for data loading, importing Source/Target tables from the respective databases and
flat files

‘* Created complex mappings in Talend using components like: (Map, tJoin, tReplicate, tAggregateRow, tDie,
tUnique, tFlowTolterate, tSort, tFilterRow, tWarn, tContextLoad

© Utilized Big Data components like tHDFSInput, tHDFSOutput, tHiveLoad, tHivelnput, tHiveOutput, tHiveRow,
tHiveConnection

‘© Worked on Talend ETL and used features such as Context variables

‘* Worked on Context variables and defined contexts for database connections, file paths for easily migrating to
different environments in a project

‘+ Implemented Error handling in Talend to validate the data Integrity and data completeness for the data from the
Flat File

* Used different database related components like tDbConnection, (DbInput, (DbOutput

© Environment: Infosphere DataStage 11.5 (Designer, Manager, Director and Administrator), Teradata, DB2, ASG
Zena tool, Toad, Shell Scripts, Talend 6.4, Hive, Pig, HDFS.

09/2020 to 11/2020 Informatica IDQ Developer
Allegis Group — Hanover, MD

+ Performed the roles of ETL Informatica and Data Quality (IDQ) developer on a data warehouse initiative and was
responsible for requirements gathering, preparing mapping document, architecting end to end ETL flow, building
complex ETL procedures, developing strategy to move existing data feeds into the Data Warehouse (DW), perform
data cleansing activities using various IDQ transformations

« Extensively used Informatica Data Quality (IDQ) profiling capabilites to profile various sources, generate score
cards, create and validate rules and provided data for business analysts for creating the rules

‘* Used Informatica Data Quality transformations to parse the “Financial Advisor” and “Financial Institution”
information from Salesforce and Touchpoint systems and perform various activities such as standardization,
labelling, parsing, address validation, address suggestion, matching and consolidation to identify redundant and
duplicate information and achieve MASTER record

* Extensively worked on performance tuning of Informatica and IDQ mappings

‘* Hands on experience using query tools like TOAD, SQL Developer, PLSQL developer and Teradata SQL Assistant

‘* Built UNIX Linux shell scripts for running Informatica workflows, data cleansing, purge, delete, and data loading
and for ELT process.

10/2016 to 08/2020 ETL Developer
Blue Cross Blue Shield — City, STATE.

‘= Involved in the Analysis of the functional side of the project by interacting with functional experts to design and
write technical specifications

‘© Worked on the Architecture of ETL process and design document, and by using the Architectural design created the
source to target mapping from source to target mapping documents that involves source file, ODS Member match,
Teradata tables, Transforming the data into 837 institutional and professional files and XML’s

+ Coordinated with Business Users for requirement gathering, business analysis to understand the business
requirement

‘© Designed data flow, workflow diagrams and prepared technical documents

+ Performed analysis, design, development, Testing and deployment for Ingestion, Integration, provisioning using
Agile Methodology

Extensively used components like tWaitForFile, tIterateToFlow, tFlowTolterate, tHashoutput, tHashinput, tMap,
‘tRunjob, tJava delimiter components and Db components to create Talend jobs

‘* Developed jobs to move inbound files to HDFS file location based on monthly, weekly, daily and hourly
pattitioning

Created Data stage jobs (ETL Process) for populating the data into the Data warehouse constantly from different
source systems like ODS, flat files, scheduled the same using Data Stage Sequencer for System Integration testing

‘* Extracted data from sources like Flat Files, DB2, Oracle and Teradata

‘* Developed technical infrastructure designs, data mappings, flows and report dissemination mechanisms by
architecting Data Warehouses and Marts

‘© Designed and developed parallel jobs, server and sequence jobs using DataStage Designer

‘* Experience in using different types of stages like Transformer, Aggregator, Merge, Join, Lookup, and Sort, remove
duplicate, Funnel, Filter, Pivot, Shared containers for developing jobs

‘* Developed various bulk load and update procedures and processes using SQL

© Loader and PL/SQL and Teradata SQL Assist

‘* Developed various data connections from data source to Tableau Desktop for report and dashboard development

‘* Created Schedules and Extracted the data into Tableau Data Engine

‘* Exported data from Hive to Teradata and using Sqoop export and created the reports using the Tableau

‘= Involved in working with various kinds of data sources such as Teradata

‘ Successfully loaded files to HDFS from Teradata, and load loaded from HDFS to HIVE and exported to Teradata
based on business requirement

‘ Involved in the Documentation of the ETL phase of the project

‘* Documenting business process, lesson leamed & best practices for the project

+ Environment: Infosphere DataStage 11.5, 8.7 and 8.5 (Designer, Manager, Director and Administrator),
Teradata, DB2, ASG Zena tool, Shell Scripts, Talend 6.4, Hive, Pig, HDFS.

 

Epucation Np TRAINING

 

05/2013 Bachelor of Science: Electronics And Instrumentation
Acharya Nagarjuna University - India

05/2016 Master of Science: Computer Science
New York Institute of Technology - Old Westbury, NY